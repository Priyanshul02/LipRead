# LipRead
## ğŸ—£ï¸ LipRead: Deep Learning-Based Lip Reading System

This project implements a deep learning pipeline that performs **visual speech recognition** â€” i.e., predicting spoken words from lip movements in videos â€” using **CTC loss** and a CNN-LSTM architecture.

It is built and trained using the **GRID corpus** dataset and includes a full pipeline for:

* Preprocessing `.mpg` videos and `.align` files
* Training a model using CTC (Connectionist Temporal Classification)
* Running inference and decoding predictions
* Streamlit interface for easy interaction

---

### ğŸ“ Folder Structure

```
LipRead/
â”œâ”€â”€ LipReading_final.ipynb   # Final Colab notebook
â”œâ”€â”€ decoder.py               # Charset and decoding logic
â”œâ”€â”€ model.py                 # Model architecture using CTC
â”œâ”€â”€ train_ctc.py             # CTC training script
â”œâ”€â”€ train_model.py           # Alternate training entry point
â”œâ”€â”€ utils.py                 # Preprocessing utilities
â”œâ”€â”€ lip_detector.py          # Mouth/lip detection from video
â”œâ”€â”€ streamlit_app.py         # Web app interface
â”œâ”€â”€ models/                  # Trained model weights (add your .h5 file here)
â””â”€â”€ data/                    # Folder containing 'videos' and 'labels'
```

---

### ğŸš€ How to Run (Colab)

1. Open the notebook: [ğŸ““ LipReading\_final.ipynb](LipReading_final.ipynb)
2. Run cells to:

   * Install dependencies
   * Download and extract GRID dataset
   * Preprocess videos and alignments
   * Train model with CTC
   * Run inference on test clips

---

### ğŸ§  Model Architecture

* **3D CNN** for spatiotemporal feature extraction from video
* **Bidirectional LSTM** for temporal modeling
* **CTC Loss** for alignment-free sequence training
* Greedy decoding for converting model outputs into text

---

### ğŸ“¦ Dependencies

Install dependencies using:

```bash
pip install -r requirements.txt
```

Includes:
`tensorflow`, `keras`, `opencv-python`, `streamlit`, `imageio`, `numpy`, `scipy`, etc.

---

### ğŸ¥ Sample Input

* Video clips: `.mpg` format
* Alignments: `.align` files for word-level timing

### ğŸ§ª Output

Model returns a text sequence matching the spoken content in the silent video.

---

### ğŸŒ Optional: Streamlit App

To launch the UI:

```bash
streamlit run streamlit_app.py
```

---

### ğŸ“š Dataset

* GRID Corpus (subset)
* Format: `data/videos/*.mpg`, `data/labels/*.align`

You can download it automatically in the notebook or manually via Google Drive.

---

### ğŸ¤ Contributors

* ğŸ‘¤ **Priyanshul** ([@Priyanshul02](https://github.com/Priyanshul02))

---

### ğŸ Future Improvements

* Beam search decoding
* Add speaker adaptation
* Real-time webcam inference
* Integrate with whisper or audio-text fusion

---

Would you like me to:

* Save this as a `README.md` file and push it to your GitHub repo?
* Include images, architecture diagrams, or sample output?

Let me know!
